# Spatial mapping of COVID-19 risk

In this part of the practical, we will utilise the skills we have learned so far to produce spatial maps of COVID-19 risk. To do this we will use the [`sf`](https://r-spatial.github.io/sf/) package, which is a brilliant package that makes many spatial analysis / Geographic Information System (GIS) tasks much more straightforward to implement.

In the first part of this practical, let's produce some spatial plots of population size at the **regional** level. To do this we will have to combine information from different data sources together, hence we will need to perform various data wrangling tasks, including **joining** of tables.

Firstly, the data for **population sizes** is contained in the `pop` object we generated earlier e.g.

```{r}
pop
```

You can see various things here. Firstly, as we noted earlier, this data frame contains population estimates at different spatial scales, defined in the `Geography` column. Let's take a look at the different entries:
    
    ```{r}
unique(pop$Geography)
```

To get a feel for how these different spatial hierarchies relate to each other, we can have a look at the `Admin. geography hierarchy` worksheet in the original Excel file (`ukmidyearestimates20182019ladcodes.xls`). This is shown in Figure @ref(fig:spatialRegions).

```{r, spatialRegions, fig.cap = "Spatial hierarchy", echo = FALSE, out.width = '90%'}
include_graphics("covid/images/spatialRegions.png")
```

Here we can see that the top of the hierarchy is **country**, and then within **England** we have **regions**, and within **regions** we have **counties** and so on. The `pop` data frame we produced earlier has population counts by age and sex in all of these different spatial hierarchies.

We are going to produce some maps at the **regional** level, and hence we can begin by subsetting the `pop` data frame to extract the population counts corresponding to **regions** only.

```{task}
Extract the subset of the `pop` data frame corresponding to the `Region` estimates. Save this as a new object called `pop_regions`.
```

```{solution}

``{r}
pop_regions <- filter(pop, Geography == "Region")
``

```

Your new data frame should look like:
    
    ```{r}
pop_regions
```

Notice the `code` and `Name` columns; these correspond to **unique** codes that can be used to **join** different data sets together. Here we will join the population data to a data set containing the relevant spatial information for mapping---in this case a **regional boundary map**.

```{info, title = "Spatial data", collapsible = FALSE}
Some data will be associated with a particular geographical entities, such as spatial regions or point locations. The **locations** of these geographical entities in space are typically characterised by a **coordinate reference system** (CRS). 

A key challenge is that spatial locations are three-dimensional (the Earth is a sphere), but maps are two-dimensional, and so in order to produce a two-dimensional map it is necessary to **project** a set of 3D coordinates down to a set of 2D coordinates. There are many different ways to do this, resulting in a wide variety of different CRS systems and projections. As such, spatial data---such as point locations or polygons---require more than just a set of coordinates, since they also require the correct CRS, projections and associated meta-data, such as names and other attributes.

For more general information about spatial reference systems, see [https://en.wikipedia.org/wiki/Geographic_coordinate_system](https://en.wikipedia.org/wiki/Geographic_coordinate_system).
```

```{info, title = "Shapefiles", collapsible = FALSE}
A common format for storing spatial information and attributes associated with each spatial element is as a ["shapefile"](https://en.wikipedia.org/wiki/Shapefile). Although we refer to a "shapefile" as a singular noun, in practice it is a collection of different files with different suffixes (e.g. `.shp`, `.shx`, `.dbf`) that together provide all the necessary information for characterising the spatial system. 

There are many great packages in R for working with spatial data, but we will use the [`sf`](https://r-spatial.github.io/sf/) package (standing for "Simple Features"), which allows us to read shapefile information into R as a `tibble` object, meaning that we can use many of the data manipulation/visualisation ideas that we have come across already to produce different spatial maps in a straightforward manner.

The `sf` package provides many tools for manipulating spatial data, but here we will just use it to produce plots using default information contained in a shapefile.
```

Firstly, load the `sf` package. If it is not installed, then it can be installed in the usual way.

```{r, message = FALSE}
## load library
library(sf)
```

Now we need to download the relevant shapefile. A range of useful shapefiles associated with UK electoral and administrative regions can be found at [https://geoportal.statistics.gov.uk/](https://geoportal.statistics.gov.uk/). In this case we want to download the **region** shapefile for **2019** (since the codes in the `pop` data set correspond to the 2019 regions). This can be downloaded from:
    
    [https://geoportal.statistics.gov.uk/search?collection=Dataset&sort=name&tags=all(BDY_LAD%2CDEC_2019)](https://geoportal.statistics.gov.uk/search?collection=Dataset&sort=name&tags=all(BDY_LAD%2CDEC_2019))

For ease, I have included the relevant file here:  [Regions_(December_2019)_Boundaries_EN_BFC.zip](uploadFiles/Regions_(December_2019)_Boundaries_EN_BFC.zip).

As discussed earlier, a **shapefile** is actually a collection of files, rather than a single file. If you **unzip** the file above, this should produce a folder called `Regions_(December_2019)_Boundaries_EN_BFC`, within which are five files:
    
    * `Regions_(December_2019)_Boundaries_EN_BFC.cpg`
* `Regions_(December_2019)_Boundaries_EN_BFC.dbf`
* `Regions_(December_2019)_Boundaries_EN_BFC.prj`
* `Regions_(December_2019)_Boundaries_EN_BFC.shp`
* `Regions_(December_2019)_Boundaries_EN_BFC.shx`

These contain all of the necessary spatial information and meta-data discussed earlier. We don't have to worry about what these files actually contain, instead the `sf` package will extract all of the necessary information for us. To do this, we use the `st_read()` function, to which we pass the path to the relevant `.shp` file. 

```{r, eval = FALSE}
regions <- st_read("Regions_(December_2019)_Boundaries_EN_BFC/Regions_(December_2019)_Boundaries_EN_BFC.shp")
regions
```

```{r, echo = FALSE}
regions <- st_read("covid/uploadFiles/Regions_(December_2019)_Boundaries_EN_BFC/Regions_(December_2019)_Boundaries_EN_BFC.shp")
regions
```

One thing you should notice is that the `regions` object we have just created is a special `tibble` object of class `sf`, and contains all the meta-data associated with each spatial region (e.g. `rgn19cd`, `rgn19nm`, `long`, `lat` etc.), as well as a special `geometry` column.

```{info, title = "Note", collapsible = FALSE}
The `geometry` column of an `sf` object contains the necessary spatial information; in this case each element of the column contains the spatial coordinates of a polygon that defines the associated spatial region. This column is not a simple R object anymore. In fact it is a `list`, where each element of the list is a special object called a `MULTIPOLYGON` object. This is possible because `data.frame` objects (and `tibble` objects) are simply `list` objects under-the-hood, and thus developers have recently cottoned onto the fact that lists of arbitrary objects can be included as columns in data frames, thus extending their utility.

In this case we do not have to worry about the `geometry` column; but it is important to note that there is a special geom function in `ggplot2`---called `geom_sf()`---that has been written specifically to facilitate the plotting of these spatial components as shown below.
```

Let's plot the spatial object using `geom_sf()`:
    
    ```{r}
## plot the regions object
ggplot(regions) + geom_sf()
```

You can see here that we didn't need to tell `ggplot()` to plot the `geometry` column---this is automatically recognised by the `geom_sf()` function. Note that we can set other aesthetics, such as fill and border colours in the usual way, by linking to the associated column we wish to map to. To this end let's add the relevant population sizes to the `regions` object.

You will notice that some of the columns of the `regions` object look familiar. For example, the `rgn19cd` column looks very similar to the `Code` column of our `pop_regions` object earlier, despite the column names being different. In fact these two columns should match up. However, we don't want to take this as a given, and so in the first instance let's check that they do in fact match up. We can do this in various ways, but I will use [`anti_join()`](#anti_join), which should return all entries from one table that don't match in another table.
    
    ```{info, title = "Note", collapsible = FALSE}
    To join two tables we need to use the `by` argument to define which columns we are joining by. For example, the code:
        
        ``{r, eval = FALSE}
    inner_join(table1, table2, by = "code")
    ``
    
    will join two tables, `table1` and `table2` according to the `code` column. This requires that the `code` column is **present in both data sets**. However, it is possible to join two tables by a common column, even if the column names differ. For example, if `table1` has a column called `code1`, which contains the same information as column `code2` in `table2`, then the two tables can be joined using the syntax:
        
        ``{r, eval = FALSE}
    inner_join(table1, table2, by = c("code1" = "code2"))
    ``
    
    Note that the **order** is important here; the left-hand side of the `by = c("code1" = "code2")` has to correspond to the left-hand side data set (`table1`), and visa-versa. The code:
        
        ``{r, eval = FALSE}
    inner_join(table1, table2, by = c("code2" = "code1"))
    ``
    
    would **not work** here. You can join by multiple columns by extending `by` argument e.g. `by = c("code1" = "code2", "name1" = "name2")` and so on.
    ```
    
    ```{r}
    ## check for any rows of `pop_regions` that can't be match to `regions`
    anti_join(pop_regions, regions, by = c("Code" = "rgn19cd"))
    ```
    
    ```{r}
    ## check for any rows of `regions` that can't be match to `pop_regions`
    anti_join(regions, pop_regions, by = c("rgn19cd" = "Code"))
    ```
    
    ```{task, title = "Question"}
    As an additional test you could join by `Code` **and** `Name`. Why would it not be sensible to do this na&iuml;vely here?
        ```
    
    ```{solution, title = "Answer"}
    It wouldn't be sensible to do that here because in the `pop_regions` data set the `Name` column is encoded in **upper-case**, whereas in the `regions` data frame the `rgn19nm` column is in **title-case**. Since R is *case sensitive*, these won't match even if the actual names are the same. 
    
    One option to check the names also match would be to change all the `rgn19nm` entries to upper-case (which can be done using the `toupper()` function). For example:
        
        ``{r}
    ## check for any rows of `regions` that can't be match to `pop_regions`
    test_regions <- mutate(regions, rgn19nm = toupper(rgn19nm))
    anti_join(test_regions, pop_regions, by = c("rgn19cd" = "Code", "rgn19nm" = "Name"))
    ``
    
    ``{r}
    ## check for any rows of `pop_regions` that can't be match to `regions`
    anti_join(pop_regions, test_regions, by = c("Code" = "rgn19cd", "Name" = "rgn19nm"))
    ``
    
    Here you can see that some names differ slightly, in that `East of England` in the `regions` data frame is encoded as `EAST` in the `pop_regions` data frame. This type of comparison can be useful in highlighting any obvious anomalies. Here I am happy that the `Code` and `rgn19cd` are sufficient to correctly match the regions for plotting.
    ```
    
    In this case it seems that we are able to **join** `regions` and `pop_regions` by the corresponding region code columns, and there are no mismatches.
    
    ```{info, title = "Important", collapsible = FALSE}
    Since an `sf` object is a special type of data frame, if you want to join an `sf` object to another data frame but **keep the spatial information** so that you can use it for plotting etc., then you have to ensure that the `sf` object is the **left-hand** object in any **join**. If it is the right-hand object, then the combined data frame will strip all of the spatial information out.
    ```
    
    ```{task}
    Produce a data frame that contains the overall population counts (so aggregated over males, females and age-classes) for each region and join this to the `regions` data. Save the combined data frame as `temp_regions` so that the original shapefile data is not overwritten. Produce a spatial plot of each region where the spatial regions are coloured according to the population size.
    ```
    
    ```{solution}
    
    ``{r}
    ## aggregate population sizes
    temp <- pop_regions %>%
        group_by(Code) %>%
        summarise(population = sum(population), .groups = "drop")
    
    ## join to `regions` data frame (making sure `regions` is on the
    ## left-hand side of the join to maintain spatial information)
    temp_regions <- inner_join(regions, temp, by = c("rgn19cd" = "Code"))
    
    ## plot regions coloured by population size
    ggplot(temp_regions) +
        geom_sf(aes(fill = population))
    ``
    
    ```
    
    We can also do things like produce side-by-side spatial plots of population size by region, for males and females respectively. To do this we will create a `temp_regions` data frame to hold the joined tables, which will expand the `regions` data frame to contain multiple entries (male and female populations) for each region[^rightjoin]. We will then use `facet_wrap()` to create the side-by-side plots in the usual way.
    
    [^rightjoin]: notice that we use a `right_join` here so that `regions` is the *left-hand* table but is expanded to include all entries in the right-hand table `temp`, which includes population sizes for males and females in each region
    
    ```{r}
    ## aggregate population sizes by male/female
    temp <- pop_regions %>%
        group_by(Code, sex) %>%
        summarise(population = sum(population), .groups = "drop")
    
    ## join to `regions` data frame (making sure `regions` is on the
    ## left-hand side of the join to maintain spatial information)
    temp_regions <- right_join(regions, temp, by = c("rgn19cd" = "Code"))
    
    ## plot regions coloured by population size
    ggplot(temp_regions) +
        geom_sf(aes(fill = population)) +
        facet_wrap(~sex)
    ```
    
    Not very interesting here. Let's do a similar thing but for COVID-19 deaths between between 7 March and 26 June 2020. The original `publishedweek262020.xlsx` Excel spreadsheet contains **weekly death counts by region** in cells `A77:AB85` of the `Covid-19 - Weekly registrations` worksheet, so once again we read these data into R and manipulate into 'tidy' format as before:

```{r, eval = FALSE}
## create vector of column names in date format
dates <- read_excel("publishedweek262020.xlsx",
    sheet = "Covid-19 - Weekly registrations", range = "C6:AB6") %>%
    colnames() %>%
    as.numeric() %>%
    as.Date(origin = "1899-12-30")

## read in the correct cells from the .xlsx
deaths_regions <- read_excel("publishedweek262020.xlsx",
    sheet = "Covid-19 - Weekly registrations", range = "A77:AB85",
    col_names = FALSE)
colnames(deaths_regions) <- c("Code", "Name", as.character(dates))
deaths_regions <- gather(deaths_regions, date, deaths, -Code, -Name) %>%
    mutate(date = as.Date(date)) %>%
    filter(date >= "2020-03-07" & date <= "2020-06-26") %>%
    group_by(Code) %>%
    summarise(deaths = sum(deaths), .groups = "drop")
deaths_regions
```

```{r, echo = FALSE, message = FALSE}
## create vector of column names in date format
dates <- read_excel("covid/uploadFiles/publishedweek262020.xlsx",
    sheet = "Covid-19 - Weekly registrations", range = "C6:AB6") %>%
    colnames() %>%
    as.numeric() %>%
    as.Date(origin = "1899-12-30")

## read in the correct cells from the .xlsx
deaths_regions <- read_excel("covid/uploadFiles/publishedweek262020.xlsx",
    sheet = "Covid-19 - Weekly registrations", range = "A77:AB85",
    col_names = FALSE)
colnames(deaths_regions) <- c("Code", "Name", as.character(dates))
deaths_regions <- gather(deaths_regions, date, deaths, -Code, -Name) %>%
    mutate(date = as.Date(date)) %>%
    filter(date >= "2020-03-07" & date <= "2020-06-26") %>%
    group_by(Code) %>%
    summarise(deaths = sum(deaths), .groups = "drop")
deaths_regions
```

```{task}
Produce a spatial plot of the **proportion** of total deaths in each region between 7 March and 26 June 2020.
```

```{solution}

``{r}
## generate regional proportions
temp <- mutate(deaths_regions, prop = deaths / sum(deaths))

## join to `regions` data frame (making sure `regions` is on the
## left-hand side of the join to maintain spatial information)
temp_regions <- right_join(regions, temp, by = c("rgn19cd" = "Code"))

## plot regions coloured by number of deaths
ggplot(temp_regions) +
    geom_sf(aes(fill = prop))
``

```

```{task}
Produce a spatial plot of the **population fatality risks** in each region between 7 March and 26 June 2020. What can you say about any spatial patterns you see?
```

```{solution}

``{r}
## generate regional PFRs
temp <- group_by(pop_regions, Code) %>%
    summarise(population = sum(population), .groups = "drop") %>%
    inner_join(deaths_regions, by = "Code") %>%
    mutate(PFR = deaths / population)

## join to `regions` data frame (making sure `regions` is on the
## left-hand side of the join to maintain spatial information)
temp_regions <- right_join(regions, temp, by = c("rgn19cd" = "Code"))

## plot regions coloured by number of deaths
ggplot(temp_regions) +
    geom_sf(aes(fill = PFR))
``

It looks to me like during the first COVID-19 wave the population fatality rates were highest in the North-West of England and London, and lowest in the South-West.
```